# File name: model_aks_gpu_config.yaml
# This configuration file is used to deploy a Hugging Face model on a Ray cluster on AKS with GPU.

# Make sure to increase resource requests and limits before using this example in production.
# For examples with more realistic resource configuration, see
# ray-cluster.complete.large.yaml and
# ray-cluster.autoscaler.large.yaml.
apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: translator-auto
spec:
  serviceUnhealthySecondThreshold: 300 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 300 # Config for the health check threshold for deployments. Default value is 6
  serveConfigV2: |
    applications:
      - name: app1
        import_path: autoscale_deployment.DagNode
        route_prefix: /
        runtime_env:
          working_dir: "https://github.com/Rowena2001/Final_Deployments/archive/refs/tags/1.1.zip"
          pip: ["torch==1.13.1", "transformers==4.30.2", "accelerate==0.20.3"]
        deployments:
          - name: Translator
            autoscaling_config:
              metrics_interval_s: 0.1
              min_replicas: 1
              max_replicas: 14
              look_back_period_s: 0.2
              downscale_delay_s: 0
              upscale_delay_s: 0
            graceful_shutdown_timeout_s: 1
            max_concurrent_queries: 1000
            ray_actor_options:
              num_cpus: 0.5
          - name: BasicDriver
            autoscaling_config:
              metrics_interval_s: 0.1
              min_replicas: 1
              max_replicas: 14
              look_back_period_s: 0.2
              downscale_delay_s: 0
              upscale_delay_s: 0
            graceful_shutdown_timeout_s: 1
            max_concurrent_queries: 1000
            ray_actor_options:
              num_cpus: 0.5
  # serveConfig:
  #   importPath: autoscale_deployment.DagNode
  #   runtimeEnv: |
  #     {"working_dir": "https://github.com/Rowena2001/Final_Deployments/archive/refs/tags/1.1.zip", 
  #     "pip": ["torch==1.13.1", "transformers==4.30.2", "accelerate==0.20.3"]}
  #   deployments:
  #     - name: Translator
  #       autoscalingConfig:
  #         # metrics_interval_s: 0.1
  #         'minReplicas: 1,
  #         maxReplicas: 10'
  #         # look_back_period_s: 0.2
  #         # downscale_delay_s: 0
  #         # upscale_delay_s: 0
  #       # graceful_shutdown_timeout_s: 1
  #       # max_concurrent_queries: 1000
  #       rayActorOptions:
  #         numCpus: 1
  #     - name: BasicDriver
  #       autoscalingConfig:
  #         # metrics_interval_s: 0.1
  #         'minReplicas: 1,
  #         maxReplicas: 10'
  #         # look_back_period_s: 0.2
  #         # downscale_delay_s: 0
  #         # upscale_delay_s: 0
  #       # graceful_shutdown_timeout_s: 1
  #       # max_concurrent_queries: 1000
  #       rayActorOptions:
  #         numCpus: 1
  rayClusterConfig:
    # The version of Ray you are using. Make sure all Ray containers are running this version of Ray.
    rayVersion: '2.5.0'
    # If enableInTreeAutoscaling is true, the autoscaler sidecar will be added to the Ray head pod.
    # Ray autoscaler integration is supported only for Ray versions >= 1.11.0
    # Ray autoscaler integration is Beta with KubeRay >= 0.3.0 and Ray >= 2.0.0.
    enableInTreeAutoscaling: true
    # autoscalerOptions is an OPTIONAL field specifying configuration overrides for the Ray autoscaler.
    # The example configuration shown below below represents the DEFAULT values.
    # (You may delete autoscalerOptions if the defaults are suitable.)
    autoscalerOptions:
      # upscalingMode is "Default" or "Aggressive."
      # Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.
      # Default: Upscaling is not rate-limited.
      # Aggressive: An alias for Default; upscaling is not rate-limited.
      upscalingMode: Default
      # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
      idleTimeoutSeconds: 60
      # image optionally overrides the autoscaler's container image.
      # If instance.spec.rayVersion is at least "2.0.0", the autoscaler will default to the same image as
      # the ray container. For older Ray versions, the autoscaler will default to using the Ray 2.0.0 image.
      ## image: "my-repo/my-custom-autoscaler-image:tag"
      # imagePullPolicy optionally overrides the autoscaler container's default image pull policy (IfNotPresent).
      imagePullPolicy: IfNotPresent
      # Optionally specify the autoscaler container's securityContext.
      securityContext: {}
      env: []
      envFrom: []
      # resources specifies optional resource request and limit overrides for the autoscaler container.
      # The default autoscaler resource limits and requests should be sufficient for production use-cases.
      # However, for large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "500m"
          memory: "512Mi"
    # Ray head pod template
    headGroupSpec:
      serviceType: ClusterIP # optional
      # the following params are used to complete the ray start: ray start --head --block ...
      rayStartParams:
        dashboard-host: '0.0.0.0'
        # block: 'true'
        # num-cpus: '1' # can be auto-completed from the limits
        # Use `resources` to optionally specify custom resource annotations for the Ray node.
        # The value of `resources` is a string-integer mapping.
        # Currently, `resources` must be provided in the specific format demonstrated below:
        # resources: '"{\"Custom1\": 1, \"Custom2\": 5}"'
      #pod template
      template:
        spec:
          containers:
          # The Ray head container
          - name: ray-head
            image: rayproject/ray:2.5.0
            ports:
              - containerPort: 6379
                name: gcs-server
              - containerPort: 8265 # Ray dashboard
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8000
                name: serve
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            # The resource requests and limits in this config are too small for production!
            # For an example with more realistic resource configuration, see
            # ray-cluster.autoscaler.large.yaml.
            # It is better to use a few large Ray pod than many small ones.
            # For production, it is ideal to size each Ray pod to take up the
            # entire Kubernetes node on which it is scheduled.
            resources:
              limits:
                cpu: "3"
                memory: "3G"
              requests:
                # For production use-cases, we recommend specifying integer CPU reqests and limits.
                # We also recommend setting requests equal to limits for both CPU and memory.
                # For this example, we use a 500m CPU request to accomodate resource-constrained local
                # Kubernetes testing environments such as KinD and minikube.
                cpu: "500m"
                # The rest state memory usage of the Ray head node is around 1Gb. We do not
                # recommend allocating less than 2Gb memory for the Ray head pod.
                # For production use-cases, we recommend allocating at least 8Gb memory for each Ray container.
                memory: "2G"
          # tolerations are used to schedule pods if the node has the matching taints (i.e. AKS nodepool taints)
          tolerations:
            - effect: NoSchedule
              key: sku
              value: gpu
              operator: Equal
    workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 1
      minReplicas: 1
      maxReplicas: 10
      # logical group name, for this called small-group, also can be functional
      groupName: small-group
      # If worker pods need to be added, we can increment the replicas.
      # If worker pods need to be removed, we decrement the replicas, and populate the workersToDelete list.
      # The operator will remove pods from the list until the desired number of replicas is satisfied.
      # If the difference between the current replica count and the desired replicas is greater than the
      # number of entries in workersToDelete, random worker pods will be deleted.
      #scaleStrategy:
      #  workersToDelete:
      #  - raycluster-complete-worker-small-group-bdtwh
      #  - raycluster-complete-worker-small-group-hv457
      #  - raycluster-complete-worker-small-group-k8tj7
      # the following params are used to complete the ray start: ray start --block ...
      rayStartParams: {}
      template:
        spec:
          # initContainers:
          # the env var $FQ_RAY_IP is set by the operator if missing, with the value of the head service name
          # - name: init
          #   image: busybox:1.28
          #   command: ['sh', '-c', "until nslookup $RAY_IP.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for K8s Service $RAY_IP; sleep 2; done"]
          containers:
          - name: ray-worker
            image: rayproject/ray:2.5.0
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            # The resource requests and limits in this config are too small for production!
            # For an example with more realistic resource configuration, see
            # ray-cluster.autoscaler.large.yaml.
            # It is better to use a few large Ray pod than many small ones.
            # For production, it is ideal to size each Ray pod to take up the
            # entire Kubernetes node on which it is scheduled.
            resources:
              limits:
                cpu: "2"
                memory: "5G"
              # For production use-cases, we recommend specifying integer CPU reqests and limits.
              # We also recommend setting requests equal to limits for both CPU and memory.
              # For this example, we use a 500m CPU request to accomodate resource-constrained local
              # Kubernetes testing environments such as KinD and minikube.
              requests:
                cpu: "500m"
                memory: "1G"
          # tolerations are used to schedule pods if the node has the matching taints (i.e. AKS nodepool taints)
          tolerations:
            - effect: NoSchedule
              key: sku
              value: gpu
              operator: Equal
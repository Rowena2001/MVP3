# File name: model_auto_config3.yaml
# This configuration file is used to deploy a Hugging Face model on a Ray cluster on AKS with autoscaling.

apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: translator-auto
spec:
  serviceUnhealthySecondThreshold: 300 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 300 # Config for the health check threshold for deployments. Default value is 6
  serveConfig:
    importPath: autoscale1.translator_app
    runtimeEnv: |
      {"working_dir": "https://github.com/Rowena2001/Final_Deployments/archive/refs/tags/1.2.zip", 
      "pip": ["torch==1.13.1", "transformers==4.30.2", "accelerate==0.20.3"]}
    deployments:
      - name: Translator
        rayActorOptions:
          numCpus: 0.5
  rayClusterConfig:
    # The version of Ray you are using. Make sure all Ray containers are running this version of Ray.
    rayVersion: '2.5.0'
    # If enableInTreeAutoscaling is true, the autoscaler sidecar will be added to the Ray head pod.
    # Ray autoscaler integration is supported only for Ray versions >= 1.11.0
    # Ray autoscaler integration is Beta with KubeRay >= 0.3.0 and Ray >= 2.0.0.
    enableInTreeAutoscaling: true
    # autoscalerOptions is an OPTIONAL field specifying configuration overrides for the Ray autoscaler.
    # The example configuration shown below below represents the DEFAULT values.
    # (You may delete autoscalerOptions if the defaults are suitable.)
    autoscalerOptions:
      # upscalingMode is "Default" or "Aggressive."
      # Conservative: Upscaling is rate-limited; the number of pending worker pods is at most the size of the Ray cluster.
      # Default: Upscaling is not rate-limited.
      # Aggressive: An alias for Default; upscaling is not rate-limited.
      upscalingMode: Default
      # idleTimeoutSeconds is the number of seconds to wait before scaling down a worker pod which is not using Ray resources.
      idleTimeoutSeconds: 60
      # image optionally overrides the autoscaler's container image.
      # If instance.spec.rayVersion is at least "2.0.0", the autoscaler will default to the same image as
      # the ray container. For older Ray versions, the autoscaler will default to using the Ray 2.0.0 image.
      ## image: "my-repo/my-custom-autoscaler-image:tag"
      # imagePullPolicy optionally overrides the autoscaler container's default image pull policy (IfNotPresent).
      imagePullPolicy: IfNotPresent
      # Optionally specify the autoscaler container's securityContext.
      securityContext: {}
      env: []
      envFrom: []
      # resources specifies optional resource request and limit overrides for the autoscaler container.
      # The default autoscaler resource limits and requests should be sufficient for production use-cases.
      # However, for large Ray clusters, we recommend monitoring container resource usage to determine if overriding the defaults is required.
      resources:
        limits:
          cpu: "900m"
          memory: "1000Mi"
        requests:
          cpu: "900m"
          memory: "1000Mi"
    # Ray head pod template
    headGroupSpec:
      serviceType: ClusterIP # optional
      # the following params are used to complete the ray start: ray start --head --block ...
      rayStartParams:
        dashboard-host: '0.0.0.0'
      #pod template
      template:
        spec:
          containers:
          # The Ray head container
          - name: ray-head
            image: rayproject/ray:2.5.0
            ports:
              - containerPort: 6379
                name: gcs-server
              - containerPort: 8265 # Ray dashboard
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8000
                name: serve
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            resources:
              limits:
                cpu: "3"
                memory: "2.5G"
              requests:
                cpu: "500m"
                memory: "2G"
          # tolerations are used to schedule pods if the node has the matching taints (i.e. AKS nodepool taints)
          tolerations:
            - effect: NoSchedule
              key: sku
              value: gpu
              operator: Equal
    workerGroupSpecs:
    # the pod replicas in this group typed worker
    - replicas: 2
      minReplicas: 2
      maxReplicas: 10
      # logical group name, for this called small-group, also can be functional
      groupName: small-group
      rayStartParams: {}
      template:
        spec:
          containers:
          - name: ray-worker
            image: rayproject/ray:2.5.0
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh","-c","ray stop"]
            resources:
              limits:
                cpu: "2"
                memory: "2G"
              requests:
                cpu: "500m"
                memory: "1G"
          # tolerations are used to schedule pods if the node has the matching taints (i.e. AKS nodepool taints)
          tolerations:
            - effect: NoSchedule
              key: sku
              value: gpu
              operator: Equal